{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1a211a-31a4-4caa-ba3b-e7e0efca56fb",
   "metadata": {
    "id": "0c1a211a-31a4-4caa-ba3b-e7e0efca56fb"
   },
   "source": [
    "<h1><center> Домашняя работа #3</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7eb14-6ab1-4e3d-ae2c-78102246b07e",
   "metadata": {
    "id": "22f7eb14-6ab1-4e3d-ae2c-78102246b07e"
   },
   "source": [
    "Задача:\n",
    "\n",
    "- еализуйте алгоритм А2С (Advanced Actor Critic)\n",
    "- обучите агента в среде Car Racing;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69a97b-473f-4ed3-a14e-f7dc3891e205",
   "metadata": {
    "id": "ec69a97b-473f-4ed3-a14e-f7dc3891e205"
   },
   "source": [
    "Описание задачи на сайте Gymnasium ([ссылка](https://gymnasium.farama.org/environments/box2d/lunar_lander/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa295485-0d74-4554-9d22-a7b399341033",
   "metadata": {
    "id": "fa295485-0d74-4554-9d22-a7b399341033"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MCvrGnEH-I2P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCvrGnEH-I2P",
    "outputId": "731fa3e3-f669-44f3-8a63-2e53abfb6f1c"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4958e80b-33b8-4e4d-ab85-e8b088b91e58",
   "metadata": {
    "id": "4958e80b-33b8-4e4d-ab85-e8b088b91e58"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d5ca49-7e2c-489d-84d5-c235a28b29a0",
   "metadata": {
    "id": "32d5ca49-7e2c-489d-84d5-c235a28b29a0"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from tqdm import trange\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import (\n",
    "    DummyVecEnv,\n",
    "    VecMonitor,\n",
    "    VecFrameStack,\n",
    "    VecTransposeImage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525bf911-5a79-4a87-9a7e-83da52e3a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fbf708-8acf-4e2a-ada4-443ea1ac5a2e",
   "metadata": {
    "id": "46fbf708-8acf-4e2a-ada4-443ea1ac5a2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\reinforcement_learning\\notebooks\\..\\src\\actor_critic.py:15: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.torch_utils import get_device\n",
    "from src.actor_critic import (\n",
    "    ActorNet,\n",
    "    ValueNet,\n",
    "    transform_state_to_tensor,\n",
    "    compute_cumulative_rewards,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fa3845-b42c-46aa-a1f7-9912691de4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff628a08-df21-4209-8789-2be403d184ed",
   "metadata": {
    "id": "ff628a08-df21-4209-8789-2be403d184ed"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba761d-d05e-4492-b134-3d02fe94777b",
   "metadata": {
    "id": "f5ba761d-d05e-4492-b134-3d02fe94777b"
   },
   "source": [
    "**Observation space:**\n",
    "\n",
    "A top-down 96x96 RGB image of the car and race track.\n",
    "\n",
    "**Actions:**\n",
    "\n",
    "- 0: steering, -1 is full left, +1 is full right\n",
    "- 1: gas\n",
    "- 2: braking\n",
    "\n",
    "The three numbers (in order) are:\n",
    "\n",
    "1. Steering\n",
    "   - Range: [-1.0, 1.0]\n",
    "   - Negative values: turn left\n",
    "   - Positive values: turn right\n",
    "2. Acceleration (Gas)\n",
    "   -  Range: [0.0, 1.0]\n",
    "   - 0 = no acceleration\n",
    "   - 1 = full acceleration\n",
    "3. Brake\n",
    "    -  Range: [0.0, 1.0]\n",
    "   - 0 = no braking\n",
    "   - 1 = full braking\n",
    "\n",
    "---\n",
    "\n",
    "Example Actions:\n",
    "\n",
    "- [0.0, 0.5, 0.0] → Go straight, accelerate at 50% power, no brake.\n",
    "- [-0.8, 0.1, 0.0] → Sharp left turn, low acceleration.\n",
    "- [0.3, 0.0, 0.7] → Gentle right turn, no gas, brake at 70%.\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "The reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles visited in the track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06NVw_Hg-DQ3",
   "metadata": {
    "id": "06NVw_Hg-DQ3"
   },
   "outputs": [],
   "source": [
    "# %pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qJxtajHf-FTy",
   "metadata": {
    "id": "qJxtajHf-FTy"
   },
   "outputs": [],
   "source": [
    "# %pip install Box2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8f7fc-271f-41fd-aaad-ce9b4df2d075",
   "metadata": {
    "id": "34d8f7fc-271f-41fd-aaad-ce9b4df2d075",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## Версия в stable_baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df5b1e-095f-4a09-8630-ebe494ccc91a",
   "metadata": {},
   "source": [
    "Мы используем CnnPolicy, так как нам нужна сверточная нейросеть для обработки изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f529e9e-d8c8-45e7-b08b-87a110b24fd9",
   "metadata": {},
   "source": [
    "Обучение идет, но нужно очень много шагов. Нормально не работает. -> Код закоментирован"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6914005-bf1b-4d76-9d51-6f7f55b04315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = make_vec_env(\n",
    "#     env_id=\"CarRacing-v3\",\n",
    "#     n_envs=4,\n",
    "#     env_kwargs={\"continuous\": True, 'max_episode_steps': 1_000},\n",
    "#     vec_env_cls=DummyVecEnv\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab4935f-fee4-49eb-865e-fb20e8ad2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = VecMonitor(env)\n",
    "# env = VecFrameStack(env, n_stack=4)\n",
    "# env = VecTransposeImage(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad25787-2f43-45cf-a2e5-d0e1a3cefe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = A2C(\n",
    "#     policy=\"CnnPolicy\",\n",
    "#     n_steps=512,\n",
    "#     gamma=0.99,\n",
    "#     learning_rate=3e-4,\n",
    "#     max_grad_norm=0.5,\n",
    "#     use_rms_prop=True,\n",
    "#     vf_coef=0.25,\n",
    "#     ent_coef=0.01,\n",
    "#     gae_lambda=0.95,\n",
    "#     normalize_advantage=True,\n",
    "#     tensorboard_log=None,\n",
    "#     env=env,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd734f3d-fd53-4e31-bfa4-d72d2e42b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.learn(\n",
    "#     total_timesteps=250_000,\n",
    "#     progress_bar=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "122eda0b-709d-4e6b-99f6-d34a53e09d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir ./a2c_carracing_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e9c69c-a245-4451-b26d-b951b1644257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"car_racing_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1f2f56-369f-413e-9cd5-d88050ac6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = A2C.load(\"car_racing_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3f9eb07-23cc-42c6-a983-3fc42839c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_agent_env():\n",
    "#     def _init():\n",
    "#         env = gym.make(\n",
    "#             \"CarRacing-v3\",\n",
    "#             continuous=True,\n",
    "#             domain_randomize=False,\n",
    "#             lap_complete_percent=0.95,\n",
    "#             max_episode_steps=5_000,\n",
    "#             render_mode=\"rgb_array\",\n",
    "#         )\n",
    "#         return env\n",
    "#     env = DummyVecEnv([_init])\n",
    "#     env = VecFrameStack(env, n_stack=4)\n",
    "#     env = VecTransposeImage(env)\n",
    "#     return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60cd1101-c067-461c-8d01-13822b74af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_env = create_agent_env()\n",
    "# render_env = gym.make(\n",
    "#     \"CarRacing-v3\",\n",
    "#     continuous=True,\n",
    "#     domain_randomize=False,\n",
    "#     lap_complete_percent=0.95,\n",
    "#     max_episode_steps=5_000,\n",
    "#     render_mode=\"human\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03e6b2fb-246e-4fdf-9675-7c612409969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_agent = agent_env.reset()\n",
    "# obs_render, _ = render_env.reset()\n",
    "\n",
    "# done = False\n",
    "# score = 0\n",
    "\n",
    "# while not done:\n",
    "#     action, _ = model.predict(obs_agent, deterministic=True)\n",
    "#     obs_agent, reward, done, _ = agent_env.step(action)\n",
    "#     _, _, _,_, _ = render_env.step(action[0])\n",
    "#     score += reward[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1a74883-db9c-4a1b-bc11-85fa8b82e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_env.close()\n",
    "# render_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7468c-5260-4203-b4de-bebdd36f995e",
   "metadata": {},
   "source": [
    "---\n",
    "## Свой класс"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2140c-0cea-41b6-9f3b-f5979c735e7f",
   "metadata": {
    "id": "27d2140c-0cea-41b6-9f3b-f5979c735e7f"
   },
   "source": [
    "## Реализация:\n",
    "1. Инициализируем случайным образом сети политики (actor) $\\pi^{\\mu}(a|s)|_{\\theta^{\\mu}}$ и V-функции (critic) $V^{\\theta}(s)|_{\\theta^{V}}$ с весами $\\theta^V$ и $\\theta^{\\mu}$ и целевые сети $V'$ и $\\pi'$: $\\theta^{V'} \\gets \\theta^V$ и $\\theta^{\\mu'} \\gets \\theta^{\\mu}$\n",
    "2. Устанавливаем число эпизодов обучения $M$ и для каждого эпизода выполняем:\n",
    "3. Проходим траекторию, пока не достигнем конечного состояния.\n",
    "    - Находясь в состоянии $s_t$ действуем в силу текущей политики и выбираем действие $a_t = \\pi^{\\mu}(s_t)|_{\\theta^{\\mu}}$\n",
    "    - Выполняем действие $a_t$ и переходим в состояние $s_{t+1}$ и получаем награду $r_t$\n",
    "    - В состоянии $s_{t+1}$ действуя в силу текущей политики выбираем действие $a_{t+1} = \\pi^{\\mu}(s_{t+1})|_{\\theta^{\\mu}}$\n",
    "    - Вычисляем $Loss(\\theta^V)=\\big( r_t + \\gamma V^{\\theta}(s_{t+1}) - V^{\\theta}(s_t) \\big)^2$\n",
    "    - Вычисляем $Loss(\\theta^{\\mu}) = \\ln{\\pi^{\\mu}(a_t|s_t)}(r_t + \\gamma V^{\\theta}(s_{t+1}) - V^{\\theta}(s_t))$\n",
    "    - Обновляем веса: </br>\n",
    "    __Внимание!__ У V-функции мы ___минимизируем___ веса, а в политике ___максимизируем_!__ </br>\n",
    "      $\\quad \\quad \\theta^V \\gets \\theta^V - \\alpha \\nabla_{\\theta^V}Loss(\\theta^V)$, </br>\n",
    "      $\\quad \\quad \\theta^{\\mu} \\gets \\theta^{\\mu} + \\beta \\nabla_{\\theta^{\\mu}}Loss(\\theta^{\\mu})$ \n",
    "    - Обновляем целевые сети: </br>\n",
    "    $\\quad \\quad \\theta^{V'} \\gets \\tau \\theta^V + (1 - \\tau) \\theta^{V'}$, </br>\n",
    "    $\\quad \\quad \\theta^{\\mu'} \\gets \\tau \\theta^{\\mu} + (1 - \\tau) \\theta^{\\mu'}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa53019a-9db2-4d0d-b4db-ff83f4a1947a",
   "metadata": {
    "id": "aa53019a-9db2-4d0d-b4db-ff83f4a1947a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"CarRacing-v3\",\n",
    "    continuous=True,\n",
    "    domain_randomize=False,\n",
    "    lap_complete_percent=0.95,\n",
    "    max_episode_steps=1_000,\n",
    "    # render_mode=\"human\",  # Раскомментируйте, чтобы увидеть игру\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18271d-ca54-4985-a588-44009ddf78f9",
   "metadata": {},
   "source": [
    "Чтобы использовать сразу несколько изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f4365e8-977c-4d8e-a5e1-a14c817a1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.FrameStackObservation(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bafbacd-ff98-4300-89bc-820a083fcf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (4, 96, 96, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060d2129-4368-40b3-b160-5a9db790f7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32689527 0.5707701  0.832319  ]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11e0187-7199-43d7-934c-fb6f58573752",
   "metadata": {
    "id": "e11e0187-7199-43d7-934c-fb6f58573752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cpu\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cac3b3a-5661-46fe-a4ed-7339e6ceda6c",
   "metadata": {
    "id": "4cac3b3a-5661-46fe-a4ed-7339e6ceda6c"
   },
   "outputs": [],
   "source": [
    "# Основные параметры RL\n",
    "gamma = torch.tensor(0.99).to(device)  # discount_factor\n",
    "num_episodes = 1500\n",
    "epsilon = 0.9  # probability of random action\n",
    "epsilon_decay = 0.001  # eps_t = eps_t-1 * (1 - epsilon_decay)\n",
    "# Основные параметры DL\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c27b2a61-c4d3-4406-a160-d5daf7d50a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model = ActorNet().to(device)\n",
    "value_model = ValueNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3e8646b-7422-4de2-8537-d3477c7ae59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = transform_state_to_tensor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4bd2fc9-9f7e-4c3b-ba40-790022eda9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_model.get_action_and_log_prob(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f470288-e10e-42a4-b16d-55c8f1a5d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_actor = torch.optim.AdamW(actor_model.parameters(), lr=lr, fused=True)\n",
    "opt_value = torch.optim.AdamW(value_model.parameters(), lr=lr, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03a1ca65-31c2-4fcd-ad0d-98e7e6617232",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03a1ca65-31c2-4fcd-ad0d-98e7e6617232",
    "outputId": "5e3eb2ee-1f99-41f1-fe6f-de3a6b2df01b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 1/1500 [02:24<60:10:38, 144.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     18\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m actor_model\u001b[38;5;241m.\u001b[39mget_action_and_log_prob(state)\n\u001b[1;32m---> 20\u001b[0m state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m     23\u001b[0m actions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\stateful_observation.py:425\u001b[0m, in \u001b[0;36mFrameStackObservation.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and info from the environment\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m    428\u001b[0m     updated_obs \u001b[38;5;241m=\u001b[39m deepcopy(\n\u001b[0;32m    429\u001b[0m         concatenate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs)\n\u001b[0;32m    430\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:562\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS\n\u001b[1;32m--> 562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_pixels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m step_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    565\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:641\u001b[0m, in \u001b[0;36mCarRacing._render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# showing stats\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_indicators(WINDOW_W, WINDOW_H)\n\u001b[1;32m--> 641\u001b[0m font \u001b[38;5;241m=\u001b[39m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFont\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_font\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m text \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%04i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward, \u001b[38;5;28;01mTrue\u001b[39;00m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    643\u001b[0m text_rect \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mget_rect()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward_records = []\n",
    "\n",
    "for episode in trange(num_episodes):\n",
    "\n",
    "    done = False\n",
    "    visited_states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Играем\n",
    "    while not done:\n",
    "\n",
    "        state = transform_state_to_tensor(state)\n",
    "        visited_states.append(state)  # add state to states list\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, _ = actor_model.get_action_and_log_prob(state)\n",
    "\n",
    "        state, reward, terminated, truncated, _ = env.step(action.detach().numpy())\n",
    "\n",
    "        done = terminated or truncated\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Train value model\n",
    "    opt_value.zero_grad()\n",
    "    cum_rewards = compute_cumulative_rewards(rewards, gamma)\n",
    "\n",
    "    predicted_cum_rewards = []\n",
    "    for state in visited_states:\n",
    "        predicted_cum_rewards.append(value_model(state))\n",
    "\n",
    "    predicted_cum_rewards = torch.cat(predicted_cum_rewards, dim=0)\n",
    "\n",
    "    value_model_loss = F.mse_loss(\n",
    "        predicted_cum_rewards,\n",
    "        torch.tensor(cum_rewards, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "    value_model_loss.backward()\n",
    "\n",
    "    opt_value.step()\n",
    "\n",
    "    # Train actor model\n",
    "    opt_actor.zero_grad()\n",
    "    advantages = (torch.tensor(cum_rewards) - predicted_cum_rewards).detach()\n",
    "\n",
    "    pi_loss = 0\n",
    "    for i, state in enumerate(visited_states):\n",
    "        _, log_prob = actor_model.get_action_and_log_prob(state)\n",
    "        pi_loss += log_prob * advantages[i]\n",
    "\n",
    "    pi_loss.backward()\n",
    "\n",
    "    opt_actor.step()\n",
    "\n",
    "    reward_records.append(sum(rewards))\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Mean for last 10 episodes is {np.mean(reward_records[-10:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e80b31-3b3f-4d2a-8711-3693bfd732c9",
   "metadata": {
    "id": "31e80b31-3b3f-4d2a-8711-3693bfd732c9"
   },
   "source": [
    "## Training graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a164cb3-dee4-4216-bde4-816e65cf236e",
   "metadata": {
    "id": "0a164cb3-dee4-4216-bde4-816e65cf236e"
   },
   "outputs": [],
   "source": [
    "table = pd.DataFrame(rewards, columns=[\"steps\", \"reward\"])\n",
    "# table = table.iloc[2_000:, :]  # remove exploratory_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b6a63-f5c3-4351-8d1e-c71d3fd35fd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "e26b6a63-f5c3-4351-8d1e-c71d3fd35fd4",
    "outputId": "9c259ad5-badf-41a5-84f5-b1db64394792"
   },
   "outputs": [],
   "source": [
    "plt.plot(table.index, table[\"reward\"].rolling(100).mean())\n",
    "plt.xlabel(\"Training episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Average reward per 100 episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y4A023hEG31F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "y4A023hEG31F",
    "outputId": "6a094d00-fd5d-455d-f45c-c76fbf0c52c1"
   },
   "outputs": [],
   "source": [
    "plt.plot(table.index, table[\"steps\"].rolling(100).mean())\n",
    "plt.xlabel(\"Training episode\")\n",
    "plt.ylabel(\"steps\")\n",
    "plt.title(\"Average steps per 100 episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf60fcc-1113-4161-9348-3a5a72627b81",
   "metadata": {
    "id": "6bf60fcc-1113-4161-9348-3a5a72627b81"
   },
   "source": [
    "## Анимация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c5a274-e857-4301-a17f-55b93e78369e",
   "metadata": {
    "id": "d1c5a274-e857-4301-a17f-55b93e78369e"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"CarRacing-v3\",\n",
    "    continuous=True,\n",
    "    domain_randomize=False,\n",
    "    lap_complete_percent=0.95,\n",
    "    max_episode_steps=5_000,\n",
    "    render_mode=\"human\",  # Раскомментируйте, чтобы увидеть игру\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df198f32-9cdc-4341-940f-45e6308735c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850117a0-f0d8-4533-b842-fbaca3f29e5c",
   "metadata": {
    "id": "850117a0-f0d8-4533-b842-fbaca3f29e5c"
   },
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "state, _ = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "# n_actions = env.action_space.n\n",
    "# n_observations = len(state)\n",
    "\n",
    "while not done:\n",
    "    env.render()  # Раскомментируйте, чтобы увидеть игру\n",
    "    # with torch.no_grad():\n",
    "    #     # best action\n",
    "    #     action = policy_net_2(state).argmax()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    state = next_state\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    score += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Score is: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa91eb-d44d-4603-bff2-a79478d9733f",
   "metadata": {
    "id": "ebaa91eb-d44d-4603-bff2-a79478d9733f"
   },
   "outputs": [],
   "source": [
    "torch.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964da7e-d45b-4269-a0b6-5eb2d826c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c0198-f5fd-4d7a-bcca-493e6a895c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
