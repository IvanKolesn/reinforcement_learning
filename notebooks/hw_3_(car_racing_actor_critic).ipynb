{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0c1a211a-31a4-4caa-ba3b-e7e0efca56fb",
      "metadata": {
        "id": "0c1a211a-31a4-4caa-ba3b-e7e0efca56fb"
      },
      "source": [
        "<h1><center> Домашняя работа #3</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f7eb14-6ab1-4e3d-ae2c-78102246b07e",
      "metadata": {
        "id": "22f7eb14-6ab1-4e3d-ae2c-78102246b07e"
      },
      "source": [
        "Задача:\n",
        "\n",
        "- еализуйте алгоритм А2С (Advanced Actor Critic)\n",
        "- обучите агента в среде Car Racing;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec69a97b-473f-4ed3-a14e-f7dc3891e205",
      "metadata": {
        "id": "ec69a97b-473f-4ed3-a14e-f7dc3891e205"
      },
      "source": [
        "Описание задачи на сайте Gymnasium ([ссылка](https://gymnasium.farama.org/environments/box2d/lunar_lander/))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa295485-0d74-4554-9d22-a7b399341033",
      "metadata": {
        "id": "fa295485-0d74-4554-9d22-a7b399341033"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fd68c832-4f03-40c0-96e0-219178a1b0a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd68c832-4f03-40c0-96e0-219178a1b0a2",
        "outputId": "90f026df-ee36-4718-e68f-713583d73bcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "1 + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "MCvrGnEH-I2P",
      "metadata": {
        "id": "MCvrGnEH-I2P"
      },
      "outputs": [],
      "source": [
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4958e80b-33b8-4e4d-ab85-e8b088b91e58",
      "metadata": {
        "id": "4958e80b-33b8-4e4d-ab85-e8b088b91e58"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "32d5ca49-7e2c-489d-84d5-c235a28b29a0",
      "metadata": {
        "id": "32d5ca49-7e2c-489d-84d5-c235a28b29a0"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "from tqdm import trange\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Commented since the code is commented too\n",
        "# from stable_baselines3 import A2C\n",
        "# from stable_baselines3.common.callbacks import BaseCallback\n",
        "# from stable_baselines3.common.env_util import make_vec_env\n",
        "# from stable_baselines3.common.vec_env import (\n",
        "#     DummyVecEnv,\n",
        "#     VecMonitor,\n",
        "#     VecFrameStack,\n",
        "#     VecTransposeImage,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "46fbf708-8acf-4e2a-ada4-443ea1ac5a2e",
      "metadata": {
        "id": "46fbf708-8acf-4e2a-ada4-443ea1ac5a2e"
      },
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "from src.torch_utils import get_device, preprocess_state, transform_state_to_tensor\n",
        "from src.actor_critic import (\n",
        "    ActorNet,\n",
        "    ValueNet,\n",
        "    compute_returns,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "58fa3845-b42c-46aa-a1f7-9912691de4b3",
      "metadata": {
        "id": "58fa3845-b42c-46aa-a1f7-9912691de4b3"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff628a08-df21-4209-8789-2be403d184ed",
      "metadata": {
        "id": "ff628a08-df21-4209-8789-2be403d184ed"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5ba761d-d05e-4492-b134-3d02fe94777b",
      "metadata": {
        "id": "f5ba761d-d05e-4492-b134-3d02fe94777b"
      },
      "source": [
        "**Observation space:**\n",
        "\n",
        "A top-down 96x96 RGB image of the car and race track.\n",
        "\n",
        "**Actions:**\n",
        "\n",
        "- 0: steering, -1 is full left, +1 is full right\n",
        "- 1: gas\n",
        "- 2: braking\n",
        "\n",
        "The three numbers (in order) are:\n",
        "\n",
        "1. Steering\n",
        "   - Range: [-1.0, 1.0]\n",
        "   - Negative values: turn left\n",
        "   - Positive values: turn right\n",
        "2. Acceleration (Gas)\n",
        "   -  Range: [0.0, 1.0]\n",
        "   - 0 = no acceleration\n",
        "   - 1 = full acceleration\n",
        "3. Brake\n",
        "    -  Range: [0.0, 1.0]\n",
        "   - 0 = no braking\n",
        "   - 1 = full braking\n",
        "\n",
        "---\n",
        "\n",
        "Example Actions:\n",
        "\n",
        "- [0.0, 0.5, 0.0] → Go straight, accelerate at 50% power, no brake.\n",
        "- [-0.8, 0.1, 0.0] → Sharp left turn, low acceleration.\n",
        "- [0.3, 0.0, 0.7] → Gentle right turn, no gas, brake at 70%.\n",
        "\n",
        "**Rewards:**\n",
        "\n",
        "The reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles visited in the track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "06NVw_Hg-DQ3",
      "metadata": {
        "id": "06NVw_Hg-DQ3"
      },
      "outputs": [],
      "source": [
        "# %pip install swig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "qJxtajHf-FTy",
      "metadata": {
        "id": "qJxtajHf-FTy"
      },
      "outputs": [],
      "source": [
        "# %pip install Box2D"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d8f7fc-271f-41fd-aaad-ce9b4df2d075",
      "metadata": {
        "id": "34d8f7fc-271f-41fd-aaad-ce9b4df2d075",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "---\n",
        "## Версия в stable_baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38df5b1e-095f-4a09-8630-ebe494ccc91a",
      "metadata": {
        "id": "38df5b1e-095f-4a09-8630-ebe494ccc91a"
      },
      "source": [
        "Мы используем CnnPolicy, так как нам нужна сверточная нейросеть для обработки изображения"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f529e9e-d8c8-45e7-b08b-87a110b24fd9",
      "metadata": {
        "id": "2f529e9e-d8c8-45e7-b08b-87a110b24fd9"
      },
      "source": [
        "Обучение идет, но нужно очень много шагов. Нормально не работает. -> Код закоментирован"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a6914005-bf1b-4d76-9d51-6f7f55b04315",
      "metadata": {
        "id": "a6914005-bf1b-4d76-9d51-6f7f55b04315"
      },
      "outputs": [],
      "source": [
        "# env = make_vec_env(\n",
        "#     env_id=\"CarRacing-v3\",\n",
        "#     n_envs=4,\n",
        "#     env_kwargs={\"continuous\": True, 'max_episode_steps': 1_000},\n",
        "#     vec_env_cls=DummyVecEnv\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4ab4935f-fee4-49eb-865e-fb20e8ad2e94",
      "metadata": {
        "id": "4ab4935f-fee4-49eb-865e-fb20e8ad2e94"
      },
      "outputs": [],
      "source": [
        "# env = VecMonitor(env)\n",
        "# env = VecFrameStack(env, n_stack=4)\n",
        "# env = VecTransposeImage(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bad25787-2f43-45cf-a2e5-d0e1a3cefe56",
      "metadata": {
        "id": "bad25787-2f43-45cf-a2e5-d0e1a3cefe56"
      },
      "outputs": [],
      "source": [
        "# model = A2C(\n",
        "#     policy=\"CnnPolicy\",\n",
        "#     n_steps=512,\n",
        "#     gamma=0.99,\n",
        "#     learning_rate=3e-4,\n",
        "#     max_grad_norm=0.5,\n",
        "#     use_rms_prop=True,\n",
        "#     vf_coef=0.25,\n",
        "#     ent_coef=0.01,\n",
        "#     gae_lambda=0.95,\n",
        "#     normalize_advantage=True,\n",
        "#     tensorboard_log=None,\n",
        "#     env=env,\n",
        "#     verbose=1\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cd734f3d-fd53-4e31-bfa4-d72d2e42b470",
      "metadata": {
        "id": "cd734f3d-fd53-4e31-bfa4-d72d2e42b470"
      },
      "outputs": [],
      "source": [
        "# model.learn(\n",
        "#     total_timesteps=250_000,\n",
        "#     progress_bar=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "122eda0b-709d-4e6b-99f6-d34a53e09d59",
      "metadata": {
        "id": "122eda0b-709d-4e6b-99f6-d34a53e09d59"
      },
      "outputs": [],
      "source": [
        "# %tensorboard --logdir ./a2c_carracing_tensorboard/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "28e9c69c-a245-4451-b26d-b951b1644257",
      "metadata": {
        "id": "28e9c69c-a245-4451-b26d-b951b1644257"
      },
      "outputs": [],
      "source": [
        "# model.save(\"car_racing_baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5c1f2f56-369f-413e-9cd5-d88050ac6351",
      "metadata": {
        "id": "5c1f2f56-369f-413e-9cd5-d88050ac6351"
      },
      "outputs": [],
      "source": [
        "# model = A2C.load(\"car_racing_baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c3f9eb07-23cc-42c6-a983-3fc42839c6f4",
      "metadata": {
        "id": "c3f9eb07-23cc-42c6-a983-3fc42839c6f4"
      },
      "outputs": [],
      "source": [
        "# def create_agent_env():\n",
        "#     def _init():\n",
        "#         env = gym.make(\n",
        "#             \"CarRacing-v3\",\n",
        "#             continuous=True,\n",
        "#             domain_randomize=False,\n",
        "#             lap_complete_percent=0.95,\n",
        "#             max_episode_steps=5_000,\n",
        "#             render_mode=\"rgb_array\",\n",
        "#         )\n",
        "#         return env\n",
        "#     env = DummyVecEnv([_init])\n",
        "#     env = VecFrameStack(env, n_stack=4)\n",
        "#     env = VecTransposeImage(env)\n",
        "#     return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "60cd1101-c067-461c-8d01-13822b74af83",
      "metadata": {
        "id": "60cd1101-c067-461c-8d01-13822b74af83"
      },
      "outputs": [],
      "source": [
        "# agent_env = create_agent_env()\n",
        "# render_env = gym.make(\n",
        "#     \"CarRacing-v3\",\n",
        "#     continuous=True,\n",
        "#     domain_randomize=False,\n",
        "#     lap_complete_percent=0.95,\n",
        "#     max_episode_steps=5_000,\n",
        "#     render_mode=\"human\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "03e6b2fb-246e-4fdf-9675-7c612409969e",
      "metadata": {
        "id": "03e6b2fb-246e-4fdf-9675-7c612409969e"
      },
      "outputs": [],
      "source": [
        "# obs_agent = agent_env.reset()\n",
        "# obs_render, _ = render_env.reset()\n",
        "\n",
        "# done = False\n",
        "# score = 0\n",
        "\n",
        "# while not done:\n",
        "#     action, _ = model.predict(obs_agent, deterministic=True)\n",
        "#     obs_agent, reward, done, _ = agent_env.step(action)\n",
        "#     _, _, _,_, _ = render_env.step(action[0])\n",
        "#     score += reward[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b1a74883-db9c-4a1b-bc11-85fa8b82e2e8",
      "metadata": {
        "id": "b1a74883-db9c-4a1b-bc11-85fa8b82e2e8"
      },
      "outputs": [],
      "source": [
        "# agent_env.close()\n",
        "# render_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf7468c-5260-4203-b4de-bebdd36f995e",
      "metadata": {
        "id": "4cf7468c-5260-4203-b4de-bebdd36f995e"
      },
      "source": [
        "---\n",
        "## Свой класс"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27d2140c-0cea-41b6-9f3b-f5979c735e7f",
      "metadata": {
        "id": "27d2140c-0cea-41b6-9f3b-f5979c735e7f"
      },
      "source": [
        "## Реализация:\n",
        "1. Инициализируем случайным образом сети политики (actor) $\\pi^{\\mu}(a|s)|_{\\theta^{\\mu}}$ и V-функции (critic) $V^{\\theta}(s)|_{\\theta^{V}}$ с весами $\\theta^V$ и $\\theta^{\\mu}$ и целевые сети $V'$ и $\\pi'$: $\\theta^{V'} \\gets \\theta^V$ и $\\theta^{\\mu'} \\gets \\theta^{\\mu}$\n",
        "2. Устанавливаем число эпизодов обучения $M$ и для каждого эпизода выполняем:\n",
        "3. Проходим траекторию, пока не достигнем конечного состояния.\n",
        "    - Находясь в состоянии $s_t$ действуем в силу текущей политики и выбираем действие $a_t = \\pi^{\\mu}(s_t)|_{\\theta^{\\mu}}$\n",
        "    - Выполняем действие $a_t$ и переходим в состояние $s_{t+1}$ и получаем награду $r_t$\n",
        "    - В состоянии $s_{t+1}$ действуя в силу текущей политики выбираем действие $a_{t+1} = \\pi^{\\mu}(s_{t+1})|_{\\theta^{\\mu}}$\n",
        "    - Вычисляем $Loss(\\theta^V)=\\big( r_t + \\gamma V^{\\theta}(s_{t+1}) - V^{\\theta}(s_t) \\big)^2$\n",
        "    - Вычисляем $Loss(\\theta^{\\mu}) = \\ln{\\pi^{\\mu}(a_t|s_t)}(r_t + \\gamma V^{\\theta}(s_{t+1}) - V^{\\theta}(s_t))$\n",
        "    - Обновляем веса: </br>\n",
        "    __Внимание!__ У V-функции мы ___минимизируем___ веса, а в политике ___максимизируем_!__ </br>\n",
        "      $\\quad \\quad \\theta^V \\gets \\theta^V - \\alpha \\nabla_{\\theta^V}Loss(\\theta^V)$, </br>\n",
        "      $\\quad \\quad \\theta^{\\mu} \\gets \\theta^{\\mu} + \\beta \\nabla_{\\theta^{\\mu}}Loss(\\theta^{\\mu})$\n",
        "    - Обновляем целевые сети: </br>\n",
        "    $\\quad \\quad \\theta^{V'} \\gets \\tau \\theta^V + (1 - \\tau) \\theta^{V'}$, </br>\n",
        "    $\\quad \\quad \\theta^{\\mu'} \\gets \\tau \\theta^{\\mu} + (1 - \\tau) \\theta^{\\mu'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e11e0187-7199-43d7-934c-fb6f58573752",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e11e0187-7199-43d7-934c-fb6f58573752",
        "outputId": "c0fd5ffb-1cc3-42ff-9592-163152280875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используемое устройство: cuda\n"
          ]
        }
      ],
      "source": [
        "device = get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4cac3b3a-5661-46fe-a4ed-7339e6ceda6c",
      "metadata": {
        "id": "4cac3b3a-5661-46fe-a4ed-7339e6ceda6c"
      },
      "outputs": [],
      "source": [
        "# Основные параметры RL\n",
        "gamma = torch.tensor(0.99).to(device)  # discount_factor\n",
        "num_episodes = 500\n",
        "\n",
        "# Основные параметры DL\n",
        "lr = 1e-3\n",
        "max_grad_norm = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\n",
        "    \"CarRacing-v3\",\n",
        "    continuous=True,\n",
        "    domain_randomize=False,\n",
        "    lap_complete_percent=0.95,\n",
        "    max_episode_steps=1_000,\n",
        "    # render_mode=\"human\",  # Раскомментируйте, чтобы увидеть игру\n",
        ")"
      ],
      "metadata": {
        "id": "Jx0_fzSGi8rS"
      },
      "id": "Jx0_fzSGi8rS",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.wrappers.FrameStackObservation(env, 4)"
      ],
      "metadata": {
        "id": "EoZX09NojuPV"
      },
      "id": "EoZX09NojuPV",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c27b2a61-c4d3-4406-a160-d5daf7d50a5c",
      "metadata": {
        "id": "c27b2a61-c4d3-4406-a160-d5daf7d50a5c"
      },
      "outputs": [],
      "source": [
        "actor_model = ActorNet().to(device)\n",
        "value_model = ValueNet().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b3e8646b-7422-4de2-8537-d3477c7ae59c",
      "metadata": {
        "id": "b3e8646b-7422-4de2-8537-d3477c7ae59c"
      },
      "outputs": [],
      "source": [
        "# state = transform_state_to_tensor(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d4bd2fc9-9f7e-4c3b-ba40-790022eda9c7",
      "metadata": {
        "id": "d4bd2fc9-9f7e-4c3b-ba40-790022eda9c7"
      },
      "outputs": [],
      "source": [
        "# actor_model.get_action_and_log_prob(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8f470288-e10e-42a4-b16d-55c8f1a5d8af",
      "metadata": {
        "id": "8f470288-e10e-42a4-b16d-55c8f1a5d8af"
      },
      "outputs": [],
      "source": [
        "opt_actor = torch.optim.AdamW(actor_model.parameters(), lr=lr, fused=True)\n",
        "opt_value = torch.optim.AdamW(value_model.parameters(), lr=lr, fused=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a1ca65-31c2-4fcd-ad0d-98e7e6617232",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03a1ca65-31c2-4fcd-ad0d-98e7e6617232",
        "outputId": "6f955efc-24fe-4e28-d583-c4537bd1a391",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/500 [00:10<1:26:12, 10.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean for last 5 episodes is -26.82926829268291\n",
            "Mean for last 50 episodes is -26.82926829268291\n"
          ]
        }
      ],
      "source": [
        "reward_records = []\n",
        "\n",
        "for episode in trange(num_episodes):\n",
        "\n",
        "    done = False\n",
        "    visited_states = []\n",
        "    terminated_flags = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    # Play one episode (collect trajectory)\n",
        "    while not done:\n",
        "\n",
        "        state = preprocess_state(state)\n",
        "        state = transform_state_to_tensor(state, device=device)\n",
        "        visited_states.append(state.squeeze(0))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            raw_actions, transformed_actions = actor_model.get_actions(state)\n",
        "\n",
        "        state, reward, terminated, truncated, _ = env.step(\n",
        "            transformed_actions.cpu().numpy().flatten()\n",
        "        )\n",
        "\n",
        "        done = terminated or truncated\n",
        "        terminated_flags.append(done)\n",
        "        actions.append(raw_actions)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Prepare data\n",
        "    rewards_tensor = torch.tensor(rewards, device=device)\n",
        "    terminated_tensor = torch.tensor(terminated_flags, device=device)\n",
        "\n",
        "    # Train value model\n",
        "    opt_value.zero_grad()\n",
        "\n",
        "    values = value_model(torch.stack(visited_states))\n",
        "    values = values.squeeze()\n",
        "\n",
        "    returns = compute_returns(\n",
        "        rewards=rewards_tensor,\n",
        "        terminated=terminated_tensor,\n",
        "        values=values,\n",
        "        gamma=gamma,\n",
        "        device=device\n",
        "    )\n",
        "    advantages = (returns - values).detach()\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    value_model_loss = F.mse_loss(values, returns)\n",
        "    value_model_loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(value_model.parameters(), max_grad_norm)\n",
        "    opt_value.step()\n",
        "\n",
        "    # Train actor model\n",
        "    opt_actor.zero_grad()\n",
        "\n",
        "    log_probs_tensor = actor_model.get_log_prob_given_actions(\n",
        "        state=torch.stack(visited_states), raw_actions=torch.cat(actions)\n",
        "    )\n",
        "    # log_probs_tensor = torch.cat(log_probs)\n",
        "\n",
        "    policy_loss = -log_probs_tensor * advantages\n",
        "    policy_loss.sum().backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(actor_model.parameters(), max_grad_norm)\n",
        "    opt_actor.step()\n",
        "\n",
        "    reward_records.append(sum(rewards))\n",
        "\n",
        "    if episode % 5 == 0:\n",
        "        print(f\"Mean for last 5 episodes is {np.mean(reward_records[-5:])}\")\n",
        "        print(f\"Mean for last 50 episodes is {np.mean(reward_records[-50:])}\")\n",
        "\n",
        "    # Stop if mean reward for 100 episodes > 475.0\n",
        "    # Naive early stopping\n",
        "    if np.average(reward_records[-100:]) > 475.0:\n",
        "        break\n",
        "\n",
        "print(f\"\\nDone in {episode+1} episodes\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e80b31-3b3f-4d2a-8711-3693bfd732c9",
      "metadata": {
        "id": "31e80b31-3b3f-4d2a-8711-3693bfd732c9"
      },
      "source": [
        "## Training graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a164cb3-dee4-4216-bde4-816e65cf236e",
      "metadata": {
        "id": "0a164cb3-dee4-4216-bde4-816e65cf236e"
      },
      "outputs": [],
      "source": [
        "table = pd.DataFrame(reward_records, columns=[\"reward\"])\n",
        "# table = table.iloc[2_000:, :]  # remove exploratory_period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "facde196-31cd-43be-ae5a-6e8df449db2c",
      "metadata": {
        "id": "facde196-31cd-43be-ae5a-6e8df449db2c"
      },
      "outputs": [],
      "source": [
        "plt.plot(table.index, table[\"reward\"])\n",
        "plt.xlabel(\"Training episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Average reward per 100 episodes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26b6a63-f5c3-4351-8d1e-c71d3fd35fd4",
      "metadata": {
        "id": "e26b6a63-f5c3-4351-8d1e-c71d3fd35fd4"
      },
      "outputs": [],
      "source": [
        "plt.plot(table.index, table[\"reward\"].rolling(100).mean())\n",
        "plt.xlabel(\"Training episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Average reward per 100 episodes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf60fcc-1113-4161-9348-3a5a72627b81",
      "metadata": {
        "id": "6bf60fcc-1113-4161-9348-3a5a72627b81"
      },
      "source": [
        "## Анимация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c5a274-e857-4301-a17f-55b93e78369e",
      "metadata": {
        "id": "d1c5a274-e857-4301-a17f-55b93e78369e"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\n",
        "    \"CarRacing-v3\",\n",
        "    continuous=True,\n",
        "    domain_randomize=False,\n",
        "    lap_complete_percent=0.95,\n",
        "    max_episode_steps=5_000,\n",
        "    render_mode=\"human\",  # Раскомментируйте, чтобы увидеть игру\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a225cd94-3173-4429-abb7-620b2b6ba068",
      "metadata": {
        "id": "a225cd94-3173-4429-abb7-620b2b6ba068"
      },
      "outputs": [],
      "source": [
        "env = gym.wrappers.FrameStackObservation(env, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850117a0-f0d8-4533-b842-fbaca3f29e5c",
      "metadata": {
        "id": "850117a0-f0d8-4533-b842-fbaca3f29e5c",
        "scrolled": true,
        "outputId": "f0645f1d-98b7-4223-b121-007aa066c5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Используемое устройство: cpu\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     state \u001b[38;5;241m=\u001b[39m preprocess_state(state)\n\u001b[0;32m      9\u001b[0m     state \u001b[38;5;241m=\u001b[39m transform_state_to_tensor(state, device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:337\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:337\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m     )\n\u001b[1;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:337\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:303\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:599\u001b[0m, in \u001b[0;36mCarRacing.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:627\u001b[0m, in \u001b[0;36mCarRacing._render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    624\u001b[0m trans \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mVector2((scroll_x, scroll_y))\u001b[38;5;241m.\u001b[39mrotate_rad(angle)\n\u001b[0;32m    625\u001b[0m trans \u001b[38;5;241m=\u001b[39m (WINDOW_W \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m trans[\u001b[38;5;241m0\u001b[39m], WINDOW_H \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m+\u001b[39m trans[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 627\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_road\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mdraw(\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf,\n\u001b[0;32m    630\u001b[0m     zoom,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    633\u001b[0m     mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_pixels_list\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_pixels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    634\u001b[0m )\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mflip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:697\u001b[0m, in \u001b[0;36mCarRacing._render_road\u001b[1;34m(self, zoom, translation, angle)\u001b[0m\n\u001b[0;32m    695\u001b[0m poly \u001b[38;5;241m=\u001b[39m [(p[\u001b[38;5;241m0\u001b[39m], p[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m poly]\n\u001b[0;32m    696\u001b[0m color \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m color]\n\u001b[1;32m--> 697\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_colored_polygon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:785\u001b[0m, in \u001b[0;36mCarRacing._draw_colored_polygon\u001b[1;34m(self, surface, poly, color, zoom, translation, angle, clip)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m clip \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    780\u001b[0m     (\u001b[38;5;241m-\u001b[39mMAX_SHAPE_DIM \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m coord[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m WINDOW_W \u001b[38;5;241m+\u001b[39m MAX_SHAPE_DIM)\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m-\u001b[39mMAX_SHAPE_DIM \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m coord[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m WINDOW_H \u001b[38;5;241m+\u001b[39m MAX_SHAPE_DIM)\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m coord \u001b[38;5;129;01min\u001b[39;00m poly\n\u001b[0;32m    783\u001b[0m ):\n\u001b[0;32m    784\u001b[0m     gfxdraw\u001b[38;5;241m.\u001b[39maapolygon(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, poly, color)\n\u001b[1;32m--> 785\u001b[0m     \u001b[43mgfxdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilled_polygon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "device = get_device()\n",
        "\n",
        "done = False\n",
        "state, _ = env.reset()\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    state = preprocess_state(state)\n",
        "    state = transform_state_to_tensor(state, device=device)\n",
        "    with torch.no_grad():\n",
        "            _, transformed_actions = actor_model.get_actions(state)\n",
        "\n",
        "    state, reward, terminated, truncated, _ = env.step(\n",
        "        transformed_actions.cpu().numpy().flatten()\n",
        "    )\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(f\"Score is: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c964da7e-d45b-4269-a0b6-5eb2d826c541",
      "metadata": {
        "id": "c964da7e-d45b-4269-a0b6-5eb2d826c541"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9c0198-f5fd-4d7a-bcca-493e6a895c71",
      "metadata": {
        "id": "dc9c0198-f5fd-4d7a-bcca-493e6a895c71"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "34d8f7fc-271f-41fd-aaad-ce9b4df2d075"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}