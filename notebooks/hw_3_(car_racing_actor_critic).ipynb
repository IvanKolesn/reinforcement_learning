{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1a211a-31a4-4caa-ba3b-e7e0efca56fb",
   "metadata": {
    "id": "0c1a211a-31a4-4caa-ba3b-e7e0efca56fb"
   },
   "source": [
    "<h1><center> Домашняя работа #3</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7eb14-6ab1-4e3d-ae2c-78102246b07e",
   "metadata": {
    "id": "22f7eb14-6ab1-4e3d-ae2c-78102246b07e"
   },
   "source": [
    "Задача:\n",
    "\n",
    "- еализуйте алгоритм А2С (Advanced Actor Critic)\n",
    "- обучите агента в среде Car Racing;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69a97b-473f-4ed3-a14e-f7dc3891e205",
   "metadata": {
    "id": "ec69a97b-473f-4ed3-a14e-f7dc3891e205"
   },
   "source": [
    "Описание задачи на сайте Gymnasium ([ссылка](https://gymnasium.farama.org/environments/box2d/lunar_lander/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa295485-0d74-4554-9d22-a7b399341033",
   "metadata": {
    "id": "fa295485-0d74-4554-9d22-a7b399341033"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MCvrGnEH-I2P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCvrGnEH-I2P",
    "outputId": "7124ee59-84cc-471c-9ec8-c0bc6f860d36"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4958e80b-33b8-4e4d-ab85-e8b088b91e58",
   "metadata": {
    "id": "4958e80b-33b8-4e4d-ab85-e8b088b91e58"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d5ca49-7e2c-489d-84d5-c235a28b29a0",
   "metadata": {
    "id": "32d5ca49-7e2c-489d-84d5-c235a28b29a0"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from tqdm import trange\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Commented since the code is commented too\n",
    "# from stable_baselines3 import A2C\n",
    "# from stable_baselines3.common.callbacks import BaseCallback\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# from stable_baselines3.common.vec_env import (\n",
    "#     DummyVecEnv,\n",
    "#     VecMonitor,\n",
    "#     VecFrameStack,\n",
    "#     VecTransposeImage,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46fbf708-8acf-4e2a-ada4-443ea1ac5a2e",
   "metadata": {
    "id": "46fbf708-8acf-4e2a-ada4-443ea1ac5a2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\reinforcement_learning\\src\\actor_critic.py:18: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.torch_utils import get_device, transform_state_to_tensor\n",
    "from src.actor_critic import (\n",
    "    ActorNet,\n",
    "    ValueNet,\n",
    "    compute_returns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58fa3845-b42c-46aa-a1f7-9912691de4b3",
   "metadata": {
    "id": "58fa3845-b42c-46aa-a1f7-9912691de4b3"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff628a08-df21-4209-8789-2be403d184ed",
   "metadata": {
    "id": "ff628a08-df21-4209-8789-2be403d184ed"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba761d-d05e-4492-b134-3d02fe94777b",
   "metadata": {
    "id": "f5ba761d-d05e-4492-b134-3d02fe94777b"
   },
   "source": [
    "**Observation space:**\n",
    "\n",
    "A top-down 96x96 RGB image of the car and race track.\n",
    "\n",
    "**Actions:**\n",
    "\n",
    "- 0: steering, -1 is full left, +1 is full right\n",
    "- 1: gas\n",
    "- 2: braking\n",
    "\n",
    "The three numbers (in order) are:\n",
    "\n",
    "1. Steering\n",
    "   - Range: [-1.0, 1.0]\n",
    "   - Negative values: turn left\n",
    "   - Positive values: turn right\n",
    "2. Acceleration (Gas)\n",
    "   -  Range: [0.0, 1.0]\n",
    "   - 0 = no acceleration\n",
    "   - 1 = full acceleration\n",
    "3. Brake\n",
    "    -  Range: [0.0, 1.0]\n",
    "   - 0 = no braking\n",
    "   - 1 = full braking\n",
    "\n",
    "---\n",
    "\n",
    "Example Actions:\n",
    "\n",
    "- [0.0, 0.5, 0.0] → Go straight, accelerate at 50% power, no brake.\n",
    "- [-0.8, 0.1, 0.0] → Sharp left turn, low acceleration.\n",
    "- [0.3, 0.0, 0.7] → Gentle right turn, no gas, brake at 70%.\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "The reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles visited in the track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06NVw_Hg-DQ3",
   "metadata": {
    "id": "06NVw_Hg-DQ3"
   },
   "outputs": [],
   "source": [
    "# %pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qJxtajHf-FTy",
   "metadata": {
    "id": "qJxtajHf-FTy"
   },
   "outputs": [],
   "source": [
    "# %pip install Box2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8f7fc-271f-41fd-aaad-ce9b4df2d075",
   "metadata": {
    "id": "34d8f7fc-271f-41fd-aaad-ce9b4df2d075",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## Версия в stable_baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df5b1e-095f-4a09-8630-ebe494ccc91a",
   "metadata": {
    "id": "38df5b1e-095f-4a09-8630-ebe494ccc91a"
   },
   "source": [
    "Мы используем CnnPolicy, так как нам нужна сверточная нейросеть для обработки изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f529e9e-d8c8-45e7-b08b-87a110b24fd9",
   "metadata": {
    "id": "2f529e9e-d8c8-45e7-b08b-87a110b24fd9"
   },
   "source": [
    "Обучение идет, но нужно очень много шагов. Нормально не работает. -> Код закоментирован"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6914005-bf1b-4d76-9d51-6f7f55b04315",
   "metadata": {
    "id": "a6914005-bf1b-4d76-9d51-6f7f55b04315"
   },
   "outputs": [],
   "source": [
    "# env = make_vec_env(\n",
    "#     env_id=\"CarRacing-v3\",\n",
    "#     n_envs=4,\n",
    "#     env_kwargs={\"continuous\": True, 'max_episode_steps': 1_000},\n",
    "#     vec_env_cls=DummyVecEnv\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4935f-fee4-49eb-865e-fb20e8ad2e94",
   "metadata": {
    "id": "4ab4935f-fee4-49eb-865e-fb20e8ad2e94"
   },
   "outputs": [],
   "source": [
    "# env = VecMonitor(env)\n",
    "# env = VecFrameStack(env, n_stack=4)\n",
    "# env = VecTransposeImage(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad25787-2f43-45cf-a2e5-d0e1a3cefe56",
   "metadata": {
    "id": "bad25787-2f43-45cf-a2e5-d0e1a3cefe56"
   },
   "outputs": [],
   "source": [
    "# model = A2C(\n",
    "#     policy=\"CnnPolicy\",\n",
    "#     n_steps=512,\n",
    "#     gamma=0.99,\n",
    "#     learning_rate=3e-4,\n",
    "#     max_grad_norm=0.5,\n",
    "#     use_rms_prop=True,\n",
    "#     vf_coef=0.25,\n",
    "#     ent_coef=0.01,\n",
    "#     gae_lambda=0.95,\n",
    "#     normalize_advantage=True,\n",
    "#     tensorboard_log=None,\n",
    "#     env=env,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd734f3d-fd53-4e31-bfa4-d72d2e42b470",
   "metadata": {
    "id": "cd734f3d-fd53-4e31-bfa4-d72d2e42b470"
   },
   "outputs": [],
   "source": [
    "# model.learn(\n",
    "#     total_timesteps=250_000,\n",
    "#     progress_bar=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122eda0b-709d-4e6b-99f6-d34a53e09d59",
   "metadata": {
    "id": "122eda0b-709d-4e6b-99f6-d34a53e09d59"
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir ./a2c_carracing_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9c69c-a245-4451-b26d-b951b1644257",
   "metadata": {
    "id": "28e9c69c-a245-4451-b26d-b951b1644257"
   },
   "outputs": [],
   "source": [
    "# model.save(\"car_racing_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f2f56-369f-413e-9cd5-d88050ac6351",
   "metadata": {
    "id": "5c1f2f56-369f-413e-9cd5-d88050ac6351"
   },
   "outputs": [],
   "source": [
    "# model = A2C.load(\"car_racing_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9eb07-23cc-42c6-a983-3fc42839c6f4",
   "metadata": {
    "id": "c3f9eb07-23cc-42c6-a983-3fc42839c6f4"
   },
   "outputs": [],
   "source": [
    "# def create_agent_env():\n",
    "#     def _init():\n",
    "#         env = gym.make(\n",
    "#             \"CarRacing-v3\",\n",
    "#             continuous=True,\n",
    "#             domain_randomize=False,\n",
    "#             lap_complete_percent=0.95,\n",
    "#             max_episode_steps=5_000,\n",
    "#             render_mode=\"rgb_array\",\n",
    "#         )\n",
    "#         return env\n",
    "#     env = DummyVecEnv([_init])\n",
    "#     env = VecFrameStack(env, n_stack=4)\n",
    "#     env = VecTransposeImage(env)\n",
    "#     return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cd1101-c067-461c-8d01-13822b74af83",
   "metadata": {
    "id": "60cd1101-c067-461c-8d01-13822b74af83"
   },
   "outputs": [],
   "source": [
    "# agent_env = create_agent_env()\n",
    "# render_env = gym.make(\n",
    "#     \"CarRacing-v3\",\n",
    "#     continuous=True,\n",
    "#     domain_randomize=False,\n",
    "#     lap_complete_percent=0.95,\n",
    "#     max_episode_steps=5_000,\n",
    "#     render_mode=\"human\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6b2fb-246e-4fdf-9675-7c612409969e",
   "metadata": {
    "id": "03e6b2fb-246e-4fdf-9675-7c612409969e"
   },
   "outputs": [],
   "source": [
    "# obs_agent = agent_env.reset()\n",
    "# obs_render, _ = render_env.reset()\n",
    "\n",
    "# done = False\n",
    "# score = 0\n",
    "\n",
    "# while not done:\n",
    "#     action, _ = model.predict(obs_agent, deterministic=True)\n",
    "#     obs_agent, reward, done, _ = agent_env.step(action)\n",
    "#     _, _, _,_, _ = render_env.step(action[0])\n",
    "#     score += reward[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a74883-db9c-4a1b-bc11-85fa8b82e2e8",
   "metadata": {
    "id": "b1a74883-db9c-4a1b-bc11-85fa8b82e2e8"
   },
   "outputs": [],
   "source": [
    "# agent_env.close()\n",
    "# render_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7468c-5260-4203-b4de-bebdd36f995e",
   "metadata": {
    "id": "4cf7468c-5260-4203-b4de-bebdd36f995e"
   },
   "source": [
    "---\n",
    "## Свой класс"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2140c-0cea-41b6-9f3b-f5979c735e7f",
   "metadata": {
    "id": "27d2140c-0cea-41b6-9f3b-f5979c735e7f"
   },
   "source": [
    "## Реализация:\n",
    "1. Инициализируем случайным образом сети политики (actor) $\\pi^{\\mu}(a|s)|_{\\theta^{\\mu}}$ и V-функции (critic) $V^{\\theta}(s)|_{\\theta^{V}}$ с весами $\\theta^V$ и $\\theta^{\\mu}$ и целевые сети $V'$ и $\\pi'$: $\\theta^{V'} \\gets \\theta^V$ и $\\theta^{\\mu'} \\gets \\theta^{\\mu}$\n",
    "2. Устанавливаем число эпизодов обучения $M$ и для каждого эпизода выполняем:\n",
    "3. Проходим траекторию, пока не достигнем конечного состояния.\n",
    "    - Находясь в состоянии $s_t$ действуем в силу текущей политики и выбираем действие $a_t = \\pi^{\\mu}(s_t)|_{\\theta^{\\mu}}$\n",
    "    - Выполняем действие $a_t$ и переходим в состояние $s_{t+1}$ и получаем награду $r_t$\n",
    "    - В состоянии $s_{t+1}$ действуя в силу текущей политики выбираем действие $a_{t+1} = \\pi^{\\mu}(s_{t+1})|_{\\theta^{\\mu}}$\n",
    "    - Вычисляем $Loss(\\theta^V)=\\big( r_t + \\gamma V^{\\theta}(s_{t+1}) - V^{\\theta}(s_t) \\big)^2$\n",
    "    - Вычисляем $Loss(\\theta^{\\mu}) = \\ln{\\pi^{\\mu}(a_t|s_t)}(r_t + \\gamma V^{\\theta}(s_{t+1}) - V^{\\theta}(s_t))$\n",
    "    - Обновляем веса: </br>\n",
    "    __Внимание!__ У V-функции мы ___минимизируем___ веса, а в политике ___максимизируем_!__ </br>\n",
    "      $\\quad \\quad \\theta^V \\gets \\theta^V - \\alpha \\nabla_{\\theta^V}Loss(\\theta^V)$, </br>\n",
    "      $\\quad \\quad \\theta^{\\mu} \\gets \\theta^{\\mu} + \\beta \\nabla_{\\theta^{\\mu}}Loss(\\theta^{\\mu})$\n",
    "    - Обновляем целевые сети: </br>\n",
    "    $\\quad \\quad \\theta^{V'} \\gets \\tau \\theta^V + (1 - \\tau) \\theta^{V'}$, </br>\n",
    "    $\\quad \\quad \\theta^{\\mu'} \\gets \\tau \\theta^{\\mu} + (1 - \\tau) \\theta^{\\mu'}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa53019a-9db2-4d0d-b4db-ff83f4a1947a",
   "metadata": {
    "id": "aa53019a-9db2-4d0d-b4db-ff83f4a1947a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"CarRacing-v3\",\n",
    "    continuous=True,\n",
    "    domain_randomize=False,\n",
    "    lap_complete_percent=0.95,\n",
    "    max_episode_steps=1_000,\n",
    "    # render_mode=\"human\",  # Раскомментируйте, чтобы увидеть игру\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18271d-ca54-4985-a588-44009ddf78f9",
   "metadata": {
    "id": "5e18271d-ca54-4985-a588-44009ddf78f9"
   },
   "source": [
    "Чтобы использовать сразу несколько изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4365e8-977c-4d8e-a5e1-a14c817a1546",
   "metadata": {
    "id": "0f4365e8-977c-4d8e-a5e1-a14c817a1546"
   },
   "outputs": [],
   "source": [
    "env = gym.wrappers.FrameStackObservation(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bafbacd-ff98-4300-89bc-820a083fcf29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bafbacd-ff98-4300-89bc-820a083fcf29",
    "outputId": "8dac98b3-4cd7-46fd-e4b9-490ec8ba8eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (4, 96, 96, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "# Пример state\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060d2129-4368-40b3-b160-5a9db790f7e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "060d2129-4368-40b3-b160-5a9db790f7e8",
    "outputId": "5b6c8c55-f05b-426f-f0fd-c9595959de9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8034253   0.27421916  0.02389392]\n"
     ]
    }
   ],
   "source": [
    "# Пример action\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e11e0187-7199-43d7-934c-fb6f58573752",
   "metadata": {
    "id": "e11e0187-7199-43d7-934c-fb6f58573752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cpu\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cac3b3a-5661-46fe-a4ed-7339e6ceda6c",
   "metadata": {
    "id": "4cac3b3a-5661-46fe-a4ed-7339e6ceda6c"
   },
   "outputs": [],
   "source": [
    "# Основные параметры RL\n",
    "gamma = torch.tensor(0.99).to(device)  # discount_factor\n",
    "num_episodes = 250\n",
    "\n",
    "# Основные параметры DL\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c27b2a61-c4d3-4406-a160-d5daf7d50a5c",
   "metadata": {
    "id": "c27b2a61-c4d3-4406-a160-d5daf7d50a5c"
   },
   "outputs": [],
   "source": [
    "actor_model = ActorNet().to(device)\n",
    "value_model = ValueNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b3e8646b-7422-4de2-8537-d3477c7ae59c",
   "metadata": {
    "id": "b3e8646b-7422-4de2-8537-d3477c7ae59c"
   },
   "outputs": [],
   "source": [
    "# state = transform_state_to_tensor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d4bd2fc9-9f7e-4c3b-ba40-790022eda9c7",
   "metadata": {
    "id": "d4bd2fc9-9f7e-4c3b-ba40-790022eda9c7"
   },
   "outputs": [],
   "source": [
    "# actor_model.get_action_and_log_prob(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8f470288-e10e-42a4-b16d-55c8f1a5d8af",
   "metadata": {
    "id": "8f470288-e10e-42a4-b16d-55c8f1a5d8af"
   },
   "outputs": [],
   "source": [
    "opt_actor = torch.optim.AdamW(actor_model.parameters(), lr=lr, fused=True)\n",
    "opt_value = torch.optim.AdamW(value_model.parameters(), lr=lr, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "03a1ca65-31c2-4fcd-ad0d-98e7e6617232",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03a1ca65-31c2-4fcd-ad0d-98e7e6617232",
    "outputId": "a2ec90a7-a956-40d2-bd07-cf781ad3f193",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/250 [01:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m _, log_prob \u001b[38;5;241m=\u001b[39m actor_model\u001b[38;5;241m.\u001b[39mget_action_and_log_prob(torch\u001b[38;5;241m.\u001b[39mcat(visited_states))\n\u001b[0;32m     54\u001b[0m policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlog_prob \u001b[38;5;241m*\u001b[39m advantages\n\u001b[1;32m---> 55\u001b[0m \u001b[43mpolicy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(actor_model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     58\u001b[0m opt_actor\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:346\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    337\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    338\u001b[0m     (inputs,)\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[0;32m    343\u001b[0m )\n\u001b[0;32m    345\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 346\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:199\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    197\u001b[0m     out_numel_is_1 \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m     )\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[0;32m    203\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "reward_records = []\n",
    "\n",
    "for episode in trange(num_episodes):\n",
    "\n",
    "    done = False\n",
    "    visited_states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Играем\n",
    "    while not done:\n",
    "\n",
    "        state = transform_state_to_tensor(state, device=device)\n",
    "        visited_states.append(state)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, _ = actor_model.get_action_and_log_prob(state)\n",
    "\n",
    "        state, reward, terminated, truncated, _ = env.step(\n",
    "            action.cpu().numpy().flatten()\n",
    "        )\n",
    "\n",
    "        done = terminated or truncated\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Train value model\n",
    "    opt_value.zero_grad()\n",
    "\n",
    "    values = value_model(torch.cat(visited_states))\n",
    "    values = values.squeeze(1)\n",
    "\n",
    "    rewards = torch.tensor(rewards)\n",
    "\n",
    "    returns = compute_returns(\n",
    "        rewards=rewards, values=values, gamma=gamma, device=device\n",
    "    )\n",
    "\n",
    "    value_model_loss = F.mse_loss(values, returns)\n",
    "    value_model_loss.backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm_(value_model.parameters(), 0.5)\n",
    "    opt_value.step()\n",
    "\n",
    "    # Train actor model\n",
    "    advantages = (returns - values).detach()\n",
    "\n",
    "    opt_actor.zero_grad()\n",
    "\n",
    "    _, log_prob = actor_model.get_action_and_log_prob(torch.cat(visited_states))\n",
    "\n",
    "    policy_loss = -log_prob * advantages\n",
    "    policy_loss.sum().backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm_(actor_model.parameters(), 0.5)\n",
    "    opt_actor.step()\n",
    "\n",
    "    reward_records.append(sum(rewards))\n",
    "\n",
    "    if episode % 5 == 0:\n",
    "        print(f\"Mean for last 5 episodes is {np.mean(reward_records[-5:])}\")\n",
    "        print(f\"Mean for last 50 episodes is {np.mean(reward_records[-50:])}\")\n",
    "\n",
    "    # stop if mean reward for 100 episodes > 475.0\n",
    "    if np.average(reward_records[-100:]) > 475.0:\n",
    "        break\n",
    "    break\n",
    "\n",
    "# print(f\"\\nDone in {episode+1} episodes\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e80b31-3b3f-4d2a-8711-3693bfd732c9",
   "metadata": {
    "id": "31e80b31-3b3f-4d2a-8711-3693bfd732c9"
   },
   "source": [
    "## Training graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a164cb3-dee4-4216-bde4-816e65cf236e",
   "metadata": {
    "id": "0a164cb3-dee4-4216-bde4-816e65cf236e"
   },
   "outputs": [],
   "source": [
    "table = pd.DataFrame(rewards, columns=[\"steps\", \"reward\"])\n",
    "# table = table.iloc[2_000:, :]  # remove exploratory_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b6a63-f5c3-4351-8d1e-c71d3fd35fd4",
   "metadata": {
    "id": "e26b6a63-f5c3-4351-8d1e-c71d3fd35fd4"
   },
   "outputs": [],
   "source": [
    "plt.plot(table.index, table[\"reward\"].rolling(100).mean())\n",
    "plt.xlabel(\"Training episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Average reward per 100 episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y4A023hEG31F",
   "metadata": {
    "id": "y4A023hEG31F"
   },
   "outputs": [],
   "source": [
    "plt.plot(table.index, table[\"steps\"].rolling(100).mean())\n",
    "plt.xlabel(\"Training episode\")\n",
    "plt.ylabel(\"steps\")\n",
    "plt.title(\"Average steps per 100 episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf60fcc-1113-4161-9348-3a5a72627b81",
   "metadata": {
    "id": "6bf60fcc-1113-4161-9348-3a5a72627b81"
   },
   "source": [
    "## Анимация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c5a274-e857-4301-a17f-55b93e78369e",
   "metadata": {
    "id": "d1c5a274-e857-4301-a17f-55b93e78369e"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"CarRacing-v3\",\n",
    "    continuous=True,\n",
    "    domain_randomize=False,\n",
    "    lap_complete_percent=0.95,\n",
    "    max_episode_steps=5_000,\n",
    "    render_mode=\"human\",  # Раскомментируйте, чтобы увидеть игру\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df198f32-9cdc-4341-940f-45e6308735c8",
   "metadata": {
    "id": "df198f32-9cdc-4341-940f-45e6308735c8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850117a0-f0d8-4533-b842-fbaca3f29e5c",
   "metadata": {
    "id": "850117a0-f0d8-4533-b842-fbaca3f29e5c"
   },
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "state, _ = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "# n_actions = env.action_space.n\n",
    "# n_observations = len(state)\n",
    "\n",
    "while not done:\n",
    "    env.render()  # Раскомментируйте, чтобы увидеть игру\n",
    "    # with torch.no_grad():\n",
    "    #     # best action\n",
    "    #     action = policy_net_2(state).argmax()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    state = next_state\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    score += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Score is: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa91eb-d44d-4603-bff2-a79478d9733f",
   "metadata": {
    "id": "ebaa91eb-d44d-4603-bff2-a79478d9733f"
   },
   "outputs": [],
   "source": [
    "torch.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964da7e-d45b-4269-a0b6-5eb2d826c541",
   "metadata": {
    "id": "c964da7e-d45b-4269-a0b6-5eb2d826c541"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c0198-f5fd-4d7a-bcca-493e6a895c71",
   "metadata": {
    "id": "dc9c0198-f5fd-4d7a-bcca-493e6a895c71"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
